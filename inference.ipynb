{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2fd5d27-34d3-4905-bf5a-542b354a1bc2",
   "metadata": {},
   "source": [
    "# This notebook is for inference on test data given in the interview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d93321-1434-4f74-9f4a-4680680568de",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "946d30a2-9f72-49f9-867f-5c0dcf75bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from feature_engineering import FeatureEngineering\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af9a4c0-2d07-46ae-ae0e-4a4880730d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import metrics\n",
    "import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import glob\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1737994-5838-4b0a-bb75-c59586f8f0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntng/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eff593-a023-4f19-87b9-e4643d00f3c0",
   "metadata": {},
   "source": [
    "## Declare path to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a570f02-37a6-4df8-8281-09385a30b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the path to the test dataset\n",
    "path_to_test_data = 'testdata'# 'testdata'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ba139e-dc6b-425b-a5fb-d2ec971e3552",
   "metadata": {},
   "source": [
    "## Internal processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3876434a-fade-4f02-8325-712db350f024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are some internal constants\n",
    "path_to_data4dev = 'assets/data4dev.csv'\n",
    "path_to_data_scaler = 'assets/data_scaler.pkl'\n",
    "path_to_data_inf_org = 'assets/path_to_data_orig_inf.csv'\n",
    "path_to_data_inf_transformed = 'assets/path_to_data_dev_inf.csv'\n",
    "model_path = {'xgb': 'results/xgb_model.pkl'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78180fdc-84f4-4f80-aa22-9d54b29aced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = utils.load_data(nonfraud_path=glob.glob(f\"{path_to_test_data}/*_nonfraud_*.csv\"), fraud_path=glob.glob(f\"{path_to_test_data}/*_fraud*.csv\"))\n",
    "df.to_csv(path_to_data_inf_org, index=False) # we need to store this for report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec6d15-ae1c-4c61-9501-525083ceceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data\n",
    "fe = FeatureEngineering(df)\n",
    "transformed_df = fe.transform() \n",
    "transformed_df.to_csv(path_to_data_inf_transformed, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49579580-6e12-4264-959f-7db692febb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for inference\n",
    "col_to_drop = ['id', 'fraud']\n",
    "X = transformed_df.drop(col_to_drop, axis=1)\n",
    "y = transformed_df['fraud']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad0dd6-1e00-455a-9bce-cdaeb881a8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "data_scaler = joblib.load(path_to_data_scaler)\n",
    "X_standardized = data_scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1207d529-2ef7-448d-b68a-4f4a0779b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_use = 'xgb'\n",
    "clf = utils.load_model(model_path[model_to_use])\n",
    "model = model_to_use + '_test'\n",
    "\n",
    "if model_to_use in ['lr', 'dt']:\n",
    "    y_pred = clf.predict(X_standardized)\n",
    "elif model_to_use == 'xgb':\n",
    "    dtest = xgb.DMatrix(X_standardized, label=y, feature_names=X.columns.tolist())\n",
    "    y_pred_proba = clf.predict(dtest)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb18c2a-e3f4-4e67-b162-044c6622c2f1",
   "metadata": {},
   "source": [
    "# Evaluate models and generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07d77b-5f85-4c12-a93d-d7dfc6f97e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find the optimal threshold using F2 scores\n",
    "# Reference: https://www.giskard.ai/glossary/f-score#:~:text=The%20F%2D2%20score%20is,2%20*%20precision%20%2B%20recall).\n",
    "from sklearn.metrics import precision_recall_curve, fbeta_score\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y, y_pred_proba)\n",
    "\n",
    "# f2 score\n",
    "precision, recall, thresholds = precision_recall_curve(y, y_pred_proba)\n",
    "f2_scores = (5 * precision * recall) / (4 * precision + recall)\n",
    "f2_scores = np.concatenate([[0], f2_scores])  # Add 0 for the last threshold (threshold=0)\n",
    "\n",
    "# Find the threshold that maximizes the F2 score\n",
    "optimal_threshold = thresholds[np.argmax(f2_scores[1:])]  # Ignore the first threshold (for threshold=0)\n",
    "\n",
    "# Classify predictions with the optimal threshold\n",
    "y_pred = (y_pred_proba >= optimal_threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db194adb-ba13-45d5-91ec-b952eb818e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1:\", metrics.calc_f1(y, y_pred))\n",
    "print(\"AUC ROC:\", metrics.calc_auc_roc(y, y_pred))\n",
    "print(\"AUC PR:\", metrics.calc_auc_pr(y, y_pred))\n",
    "print(\"Sensitivity (Recall):\", metrics.calc_sensitivity(y, y_pred))\n",
    "print(\"Specificity:\", metrics.calc_specificity(y, y_pred))\n",
    "metrics.generate_confusion_matrix(y, y_pred, 'result_on_test/confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bfd2e1-4856-4196-a8a8-b5feee419376",
   "metadata": {},
   "source": [
    "I can see the performance on the test have some false negatives and false postives. Given the fact that Recall is 0.7, I think there is room for improvement. I already tried to find the optimal threshold using F2 score which balance precision and recall but put more focus on recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ee5341-9854-441b-ba9f-1c390a828cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The report can be found at prediction_report.csv\n",
    "if model_to_use in ['lr', 'dt']:\n",
    "    y_pred_proba = clf.predict_proba(X_standardized)\n",
    "elif model_to_use == 'xgb':\n",
    "    y_pred_proba = y_pred_proba.reshape((-1, 1))\n",
    "    y_pred_proba = np.hstack([np.zeros((y_pred_proba.shape[0], 1)), y_pred_proba])\n",
    "    \n",
    "utils.generate_prediction_report(path_to_data_inf_org, path_to_data_inf_transformed, \n",
    "                                 y, y_pred, y_pred_proba, \n",
    "                                 'result_on_test/prediction_report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f6c3c-174a-4604-bf0e-9c5da1a675df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the list of false negatives and false positives. \n",
    "# I hope we can make it better at Vipps MobilePay\n",
    "false_predictions = metrics.identify_false_predictions(path_to_data_inf_org, path_to_data_inf_transformed, y, y_pred)\n",
    "false_predictions.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea7b81-4f7e-45e1-9bab-b79789b22826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
